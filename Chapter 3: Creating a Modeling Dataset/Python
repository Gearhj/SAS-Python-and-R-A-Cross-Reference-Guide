
#Frequency distribution code:
loan_samp['loan_status'].value_counts()

############################################################################################################

#Target variable assessment:
import numpy as np
loan_samp['bad'] = np.where((loan_samp['loan_status']=='Charged Off') | (loan_samp['loan_status']=='Does not meet the credit policy. Status:Charged Off'),'1','0')

############################################################################################################

#Cross-tab frequency distribution code:
pd.crosstab(loan_samp['issue_d'], loan_samp['bad'], dropna=False)

############################################################################################################

#Convert character variable to date:
loan_samp['issue_d'] = pd.to_datetime(loan_samp['issue_d'])

############################################################################################################

#Limit date range of modeling dataset:
loan_data = loan_samp[(loan_samp['issue_d']>='2014-01-01') & (loan_samp['issue_d']<='2017-12-31')]

############################################################################################################

#Summary stats for numeric data:
pd.set_option('display.max_rows', len(loan_data.index))
pd.set_option('display.max_columns', len(loan_data.columns))
loan_data.describe()

############################################################################################################

#Histogram development with 50 bins:
import matplotlib.pyplot as plt
import pandas as pd
plt.hist(loan_data['loan_amnt'], bins=50, color='blue')

############################################################################################################

#Outlier detection and adjustment:
#select numeric data only
def filter_numeric_columns(df):
    numeric_cols = df.select_dtypes(include='number').columns
    return df[numeric_cols]
 
numeric_data = filter_numeric_columns(loan_data)
 
#infer outliers with 1.5 IQR rule
import numpy as np
def replace_outliers(arr):
    #calculate the first and third quartiles
    q1 = np.percentile(arr, 25)
    q3 = np.percentile(arr, 75)
    
    #calculate the interquartile range (IQR)
    iqr = q3 - q1
    
    #calculate the lower and upper bounds for the outliers
    lower_bound = q1 - (1.5 * iqr)
    upper_bound = q3 + (1.5 * iqr)
    
    #replace outlier values with the 1.5 IQR rule values
    arr[arr < lower_bound] = lower_bound
    arr[arr > upper_bound] = upper_bound
    
    return arr
 
outliers = replace_outliers(numeric_data)
outliers.describe()


############################################################################################################


#Feature selection through correlation analysis
# Load the necessary packages
import pandas as pd
from sklearn.feature_selection import VarianceThreshold
from scipy.stats import pearsonr
 
# Perform correlation analysis
corr_matrix = numeric_data.corr(method='pearson')
 
# Set the threshold for correlation
corr_threshold = 0.8
 
# Find the highly correlated variables and remove them
high_corr = set()
for i in range(len(corr_matrix.columns)):
    for j in range(i):
        if abs(corr_matrix.iloc[i, j]) > corr_threshold:
            colname_i = corr_matrix.columns[i]
            colname_j = corr_matrix.columns[j]
            if colname_i not in high_corr:
                high_corr.add(colname_j)
                
CORR_limit = numeric_data.drop(high_corr, axis=1)


############################################################################################################


#Filtering with wrapper method:
import pandas as pd
import numpy as np
from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression
 
# Filter numeric columns including the target variable
numeric_columns = loan_data.select_dtypes(include=[np.number]).columns
numeric_columns = numeric_columns.append(pd.Index(['bad']))  # Include the target variable
loan_data_numeric = loan_data[numeric_columns].copy()
 
# Drop rows with missing values from the copied subset
loan_data_numeric.dropna(inplace=True)
 
# Separate the features (X) and target variable (y)
X = loan_data_numeric.drop('bad', axis=1)
y = loan_data_numeric['bad']
 
# Create the model to be used for feature selection
model = LogisticRegression()
 
# Perform Recursive Feature Elimination (RFE)
rfe = RFE(model, n_features_to_select=5)  # Select top 5 features
rfe.fit(X, y)
 
# Get the selected features
selected_features = X.columns[rfe.support_]
print(selected_features)


############################################################################################################


#Filtering with embedded method:
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import SelectFromModel
 
# Filter numeric columns including the target variable
numeric_columns = loan_data.select_dtypes(include=[np.number]).columns
numeric_columns = numeric_columns.append(pd.Index(['bad']))  # Include the target variable
 
# Create a copy of the data set to avoid modifying the original DataFrame
filtered_data = loan_data[numeric_columns].copy()
 
# Remove rows with missing values
filtered_data.dropna(inplace=True)
 
# Separate the features (X) and target variable (y)
X = filtered_data.drop('bad', axis=1)
y = filtered_data['bad']
 
# Create the Random Forest classification model
model = RandomForestClassifier()
 
# Perform feature selection using embedded methods (Random Forest)
selector = SelectFromModel(model)
selector.fit(X, y)
 
# Get the selected features
selected_features = X.columns[selector.get_support()]
print(selected_features)


############################################################################################################


#Feature scaling:
import pandas as pd
from sklearn.preprocessing import StandardScaler
numeric_cols = loan_data.select_dtypes(include=['float64', 'int64']).columns
scaler = StandardScaler()
loan_data_scaled = loan_data.copy()
loan_data_scaled[numeric_cols] = scaler.fit_transform(loan_data[numeric_cols])


############################################################################################################


#Encoding categorical variables:
dummy_vars = pd.get_dummies(loan_data['home_ownership'], prefix='home_ownership')
loan_data = pd.concat([loan_data, dummy_vars], axis=1)
loan_data.drop('home_ownership', axis=1, inplace=True)


############################################################################################################


#Creating polynomial variables:
from sklearn.preprocessing import PolynomialFeatures
 
poly = PolynomialFeatures(degree=2)
loan_amnt_poly = poly.fit_transform(X[['loan_amnt']])


############################################################################################################


#Create feature interactions:
import pandas as pd
 
loan_data['DTI_INC_INTERACTION'] = loan_data['dti'] * loan_data['annual_inc']


############################################################################################################


#Dimensionality Reduction with PCA:
from sklearn.decomposition import PCA
import pandas as pd
 
data = loan_data[['total_acc', 'open_acc', 'pub_rec', 'pub_rec_bankruptcies']]
 
# Standardize the data
data_std = (data - data.mean()) / data.std()
 
# Perform PCA
pca = PCA()
pca.fit(data_std)
 
# Access the principal components
principal_components = pca.components_


############################################################################################################


#Dataset balancing - 50/50 split:
	
import pandas as pd
from sklearn.utils import resample
 
# Separate positive and negative cases
positive_cases = loan_data[loan_data['bad'] == 1]
negative_cases = loan_data[loan_data['bad'] == 0]
 
# Sample the negative cases to match the number of positive cases
negative_sampled = resample(negative_cases, replace=False, n_samples=len(positive_cases), random_state=42)
 
# Concatenate the positive and sampled negative cases
balanced_data = pd.concat([positive_cases, negative_sampled])
 
# Check frequency distribution of the target variable
balanced_data['bad'].value_counts()


############################################################################################################
############################################################################################################













































